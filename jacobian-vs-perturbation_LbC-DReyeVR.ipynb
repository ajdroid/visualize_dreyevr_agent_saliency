{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perturbation saliency on LBC agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__ # Get PyTorch and CUDA version\n",
    "torch.cuda.is_available() # Check that CUDA works\n",
    "torch.cuda.device_count() # Check how many CUDA capable devices you have\n",
    "\n",
    "# Print device human readable names\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import warnings ; warnings.filterwarnings('ignore') # mute warnings, live dangerously\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl #; mpl.use(\"Agg\") # turn this on when only saving and not showing\n",
    "import matplotlib.animation as manimation\n",
    "\n",
    "import gym, os, sys, time, argparse\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "# sys.path.append('..')\n",
    "# from visualize_dreyevr_agent_saliency.saliency import get_mask\n",
    "# from saliency import get_env_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "LBC_root = Path(\"/scratch/abhijatb/Bosch22/LbC_DReyeVR/\")\n",
    "CARLA_ROOT = Path(\"/scratch/abhijatb/Bosch22/carla.harp_p13bd/\")\n",
    "sys.path.insert(0, str(CARLA_ROOT / 'PythonAPI/carla'))\n",
    "sys.path.insert(0, str(CARLA_ROOT / 'PythonAPI/carla/dist/carla-0.9.13-py3.6-linux-x86_64.egg'))\n",
    "sys.path.insert(0, str(LBC_root))\n",
    "sys.path.insert(0, str(LBC_root / 'leaderboard'))\n",
    "sys.path.insert(0, str(LBC_root / 'leaderboard/team_code'))\n",
    "sys.path.insert(0, str(LBC_root / 'scenario_runner'))\n",
    "sys.path.insert(0, str(CARLA_ROOT / 'PythonAPI/examples')) # for DReyeVR_utils\n",
    "\n",
    "from image_agent import ImageAgent\n",
    "import torch, torchvision\n",
    "torch.cuda.set_device(0)\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(route_data_path, sampled_dataidx):\n",
    "    route_rgb_path = route_data_path / 'rgb'\n",
    "    route_rgb_pathL = route_data_path / 'rgb_left'\n",
    "    route_rgb_pathR = route_data_path / 'rgb_right'\n",
    "    route_measurements_path = route_data_path / 'measurements'\n",
    "\n",
    "    rgb_img_path = route_rgb_path / '{:04d}.png'.format(sampled_dataidx)\n",
    "    rgb_imgL_path = route_rgb_pathL / '{:04d}.png'.format(sampled_dataidx)\n",
    "    rgb_imgR_path = route_rgb_pathR / '{:04d}.png'.format(sampled_dataidx)\n",
    "    rgb_img = Image.open(str(rgb_img_path))\n",
    "    rgb_imgL = Image.open(str(rgb_imgL_path))\n",
    "    rgb_imgR = Image.open(str(rgb_imgR_path))\n",
    "\n",
    "    measure_path = route_measurements_path / '{:04d}.json'.format(sampled_dataidx)\n",
    "    with open(measure_path) as read_file:\n",
    "        json_str = read_file.readline()\n",
    "        input_data = ast.literal_eval(json_str)\n",
    "    input_data['rgb'] = rgb_img\n",
    "    input_data['rgb_left'] = rgb_imgL\n",
    "    input_data['rgb_right'] = rgb_imgR\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"/scratch/abhijatb/Bosch22/dreyevr_recordings_sensors/LBC_config\")\n",
    "routedir_iterator = data_root.iterdir()\n",
    "for route_dir in routedir_iterator:\n",
    "    break\n",
    "route_data_path = route_dir\n",
    "route_rgb_path = route_data_path / 'rgb'\n",
    "final_datapt_idx = int(sorted(list(route_rgb_path.glob('*.png')))[-1].stem)\n",
    "datapt_idcs = np.arange(final_datapt_idx)+1 # starts from 0001.png\n",
    "\n",
    "sampled_dataidx = np.random.choice(datapt_idcs)\n",
    "input_data = get_data(route_data_path, sampled_dataidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals/setup for saliency goes here\n",
    "post_h=144\n",
    "post_w=256\n",
    "# prepro = lambda img: cv2.resize(img, (post_h,post_w)).astype(np.float32).reshape(post_h,post_w)/255.\n",
    "# prepro = lambda img: img.astype(np.float32)/255.\n",
    "def prepro(img):\n",
    "    return img.astype(np.float32)/255.\n",
    "# searchlight = lambda I, mask: I*mask + gaussian_filter(I, sigma=3)*(1-mask) # choose an area NOT to blur\n",
    "# occlude = lambda I, mask: I*(1-mask) + gaussian_filter(I, sigma=3)*mask # choose an area to blur\n",
    "def occlude(I, mask):\n",
    "    return I*(1-mask) + gaussian_filter(I, sigma=3)*mask\n",
    "# what else goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fwd_img_model(dreyevr_img_agent, input_data, mask=None):\n",
    "#     tick_data = dreyevr_img_agent.offline_tick(input_data)\n",
    "    tick_data = deepcopy(input_data)\n",
    "    # order is 'rgb', 'rgb_left', 'rgb_right'\n",
    "    img = torchvision.transforms.functional.to_tensor(tick_data['image'])\n",
    "    img = img[None].cuda()\n",
    "\n",
    "    target = torch.from_numpy(tick_data['target'])\n",
    "    target = target[None].cuda()\n",
    "    _, (_, _), logits = dreyevr_img_agent.net.forward_w_logit(img, target)\n",
    "    # this is (1x4xHW) -- 4 is the num of intermediate pts being pred \n",
    "    flat_logits = logits.view(logits.shape[:-2] + (-1,)) #.detach().cpu().numpy()\n",
    "    return flat_logits    \n",
    "    \n",
    "\n",
    "def get_mask(center, size, r):\n",
    "    y,x = np.ogrid[-center[0]:size[0]-center[0], -center[1]:size[1]-center[1]]\n",
    "    keep = x*x + y*y <= 1\n",
    "    mask = np.zeros(size) ; mask[keep] = 1 # select a circle of pixels\n",
    "    mask = gaussian_filter(mask, sigma=r) # blur the circle of pixels. this is a 2D Gaussian for r=r^2=1\n",
    "    return mask/mask.max()\n",
    "\n",
    "def apply_mask(input_data, mask, interp_func, channel=0):\n",
    "    masked_data = deepcopy(input_data)\n",
    "    if channel < 3:        \n",
    "        img = masked_data['image'][..., (channel*3):(channel*3)+3]\n",
    "        # perturb input I -> I'\n",
    "        im1 = interp_func(prepro(img[...,0]).squeeze(), mask)\n",
    "        im2 = interp_func(prepro(img[...,1]).squeeze(), mask)\n",
    "        im3 = interp_func(prepro(img[...,2]).squeeze(), mask)\n",
    "        masked_data['image'][..., (channel*3):(channel*3)+3] = (np.stack((im1, im2, im3), axis=2)*255).astype(int)\n",
    "#     elif channel == 3: # actually the 3rd channel is happening inside the thing so irrelevant\n",
    "#         img = input_data['image'][..., -1]\n",
    "#         im = interp_func(prepro(img).squeeze(), mask).reshape(post_h,post_w)\n",
    "#         masked_data['image'][..., -1] = im\n",
    "    else:\n",
    "        raise ValueError(\"channel must be 0-2:rgb/left/right or 3:command heatmap\")     \n",
    "    return masked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_img_model_batch(dreyevr_img_agent, batch_data):\n",
    "    # img = torchvision.transforms.functional.to_tensor(tick_data[1:]['image'])    \n",
    "    result = [torchvision.transforms.functional.to_tensor(input_data_dict['image'])\\\n",
    "                       for input_data_dict in batch_data]\n",
    "    imgs_tensor = torch.stack(result, 0)\n",
    "    imgs_tensor = imgs_tensor.cuda()\n",
    "\n",
    "    targets_tensor = [torch.from_numpy(input_data_dict['target'])\\\n",
    "                       for input_data_dict in batch_data]\n",
    "    targets_tensor = torch.stack(targets_tensor,0)\n",
    "    targets_tensor = targets_tensor.cuda()\n",
    "\n",
    "    _, (_, _), logits = dreyevr_img_agent.net.forward_w_logit(imgs_tensor, targets_tensor)\n",
    "    flat_logits = logits.view(logits.shape[:-2] + (-1,))\n",
    "    return flat_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_masked_data_parallel(ijk, other_data):\n",
    "    i,j,k = ijk\n",
    "    input_data, r, interp_func = other_data \n",
    "    return get_and_apply_mask([i,j], input_data, r, interp_func, k)\n",
    "\n",
    "def get_and_apply_mask(center, input_data, r, interp_func, channel):\n",
    "    H, W = input_data['image'][..., (channel*3)].shape[:2]\n",
    "    mask = get_mask(center=center, size=[H, W], r=r)\n",
    "    masked_data = apply_mask(input_data, mask, interp_func=occlude, channel=channel)\n",
    "    return masked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_frame(dreyevr_img_agent, input_data, r, d, interp_func, pt_aggregate=\"leading\"):\n",
    "    # r: radius of blur\n",
    "    # d: density of scores (if d==1, then get a score for every pixel...\n",
    "    #    if d==2 then every other, which is 25% of total pixels for a 2D image)\n",
    "    # unmodified image logits\n",
    "    input_data = dreyevr_img_agent.offline_tick(input_data)\n",
    "    L = fwd_img_model(dreyevr_img_agent, input_data)    \n",
    "    scores = np.zeros((int(post_h/d)+1,int(post_w/d)+1, 3)) # saliency scores S(t,i,j)\n",
    "\n",
    "    for i in range(0,post_h,d):\n",
    "        for j in range(0,post_w,d):\n",
    "            for k in range(0,3): # this is for the channel rgb/left/right\n",
    "                mask = get_mask(center=[i,j], size=[post_h,post_w], r=r)\n",
    "                masked_data = apply_mask(input_data, mask, occlude, channel=k)\n",
    "                # masked image logits\n",
    "                l = fwd_img_model(dreyevr_img_agent, masked_data)\n",
    "                # this corresponds to \n",
    "                if pt_aggregate==\"leading\":\n",
    "                    scores[int(i/d),int(j/d), k] = (L-l)[:,:2,:].pow(2).sum().mul_(.5).data.item()\n",
    "                elif pt_aggregate==\"all\":\n",
    "                    scores[int(i/d),int(j/d), k] = (L-l).pow(2).sum().mul_(.5).data.item()\n",
    "                else:\n",
    "                    raise ValueError(\"only 'leading'(first 2) and 'all' aggregations are supported\")\n",
    "\n",
    "    pmax = scores.max()\n",
    "    scores = cv2.resize(scores, dsize=(post_w, post_h), interpolation=cv2.INTER_LINEAR).astype(np.float32)\n",
    "    smap = pmax * scores / scores.max()\n",
    "    smap = smap.astype(int)\n",
    "    return smap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_frame_batched(dreyevr_img_agent, input_data, r, d, interp_func, pt_aggregate=\"leading\", batch_size=64):\n",
    "    # r: radius of blur\n",
    "    # d: density of scores (if d==1, then get a score for every pixel...\n",
    "    #    if d==2 then every other, which is 25% of total pixels for a 2D image)\n",
    "    # unmodified image logits\n",
    "    input_data = dreyevr_img_agent.offline_tick(input_data)\n",
    "    L = fwd_img_model(dreyevr_img_agent, input_data) \n",
    "    \n",
    "    # more parallelism here\n",
    "    masked_data_arr = np.empty((int(post_h/d)+1, int(post_w/d)+1, 3), dtype=dict)\n",
    "    for i in range(0,post_h,d):\n",
    "        for j in range(0,post_w,d):\n",
    "            for k in range(0,3): # this is for the channel rgb/left/right\n",
    "                masked_data_arr[int(i/d),int(j/d), k] = get_and_apply_mask([i,j], input_data, r, interp_func, k)\n",
    "    masked_data_flat = masked_data_arr.reshape(-1)\n",
    "    \n",
    "    # aggregate batches for forward\n",
    "    num_batches = int(masked_data_flat.size/batch_size)+1\n",
    "    scores = np.zeros(shape=masked_data_flat.shape)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        if i < num_batches-1:\n",
    "            batch_data = masked_data_flat[i*batch_size:(i+1)*batch_size]        \n",
    "        else:\n",
    "            batch_data = masked_data_flat[i*batch_size:]    \n",
    "\n",
    "        flat_logits = fwd_img_model_batch(dreyevr_img_agent, batch_data)\n",
    "\n",
    "        if pt_aggregate==\"leading\":\n",
    "            score_temp = (L-flat_logits)[:,:2,:].pow(2).sum(dim=[1,2]).mul_(.5).data.tolist()\n",
    "        else:\n",
    "            score_temp = (L-flat_logits).pow(2).sum(dim=[1,2]).mul_(.5).data.tolist()\n",
    "\n",
    "        if i< num_batches-1:\n",
    "            scores[i*batch_size:(i+1)*batch_size] = score_temp\n",
    "        else:\n",
    "            scores[i*batch_size:] = score_temp\n",
    "    scores = scores.reshape(masked_data_arr.shape)\n",
    "    \n",
    "#     ijks = list(itertools.product(*[range(0,post_h,d), range(0,post_w,d), range(3)]))\n",
    "#     other_data = (input_data, r, interp_func)\n",
    "#     _args = zip(ijks, itertools.repeat(other_data))\n",
    "#     with Pool() as pool:\n",
    "#         results = pool.starmap(_get_masked_data_parallel, _args)\n",
    "    \n",
    "#     results  \n",
    "    pmax = scores.max()\n",
    "    scores = cv2.resize(scores, dsize=(post_w, post_h), interpolation=cv2.INTER_LINEAR).astype(np.float32)\n",
    "    smap = pmax * scores / scores.max()\n",
    "    smap = smap.astype(int)\n",
    "    return smap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Make saliency movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl ; mpl.use(\"Agg\")\n",
    "import matplotlib.animation as manimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set metadata for movie to be written\n",
    "def saliency_movie(path_to_conf_file, route_data_path):\n",
    "    # model load\n",
    "#     path_to_conf_file = LBC_root / \"checkpoints/imgmodel_17trainseqs_CC-1_LR-4_FT/epoch=27.ckpt\"\n",
    "    # \"checkpoints/imgmodel_17trainseqs_CC-2_LR-4_FT/epoch=47.ckpt\"\n",
    "    dreyevr_img_agent = ImageAgent(str(path_to_conf_file))\n",
    "\n",
    "    r, d = 10, 10\n",
    "    aggregate_method =\"leading\"\n",
    "    batch_size = 96\n",
    "    \n",
    "    model_name = path_to_conf_file.parents[0].stem\n",
    "    movie_title = model_name \\\n",
    "                    + \"_\" + route_data_path.stem.split('_')[1] \\\n",
    "                    + \"_\" + aggregate_method  + \".mp4\"\n",
    "\n",
    "    save_dir = \"/scratch/abhijatb/Bosch22/LbC_DReyeVR/saliency_movies/\"\n",
    "    if os.path.exists(save_dir + movie_title):\n",
    "        print(movie_title, \"already exists\")\n",
    "        return\n",
    "    else:\n",
    "        print(movie_title, \"being processed\")\n",
    "    resolution=150\n",
    "\n",
    "    start = time.time()\n",
    "    FFMpegWriter = manimation.writers['ffmpeg']\n",
    "    metadata = dict(title=movie_title, artist='ajdroid', comment='dreyevrLBC-saliency-video')\n",
    "    writer = FFMpegWriter(fps=2, metadata=metadata)\n",
    "    \n",
    "    final_datapt_idx = int(sorted(list((route_data_path/'rgb').glob('*.png')))[-1].stem)\n",
    "    datapt_idcs = np.arange(final_datapt_idx)+1 # starts from 0001.png\n",
    "    prog = '' ; total_frames = datapt_idcs[-1]\n",
    "    \n",
    "    f = plt.figure(figsize=[10*3.2, 10], dpi=resolution)\n",
    "\n",
    "    with writer.saving(f, save_dir + movie_title, resolution):\n",
    "        for i in datapt_idcs:\n",
    "            input_data = get_data(route_data_path, i)\n",
    "            smap = score_frame_batched(dreyevr_img_agent, input_data, r, d, \n",
    "                               interp_func=occlude, pt_aggregate=aggregate_method, batch_size=batch_size)\n",
    "            # image order is rgb, rgb_left, rgb_right\n",
    "            salmap_3stack = np.hstack([smap[:,:,1], smap[:,:,0], smap[:,:,2]])\n",
    "            img_3stack = np.hstack([input_data['image'][...,3:6],\n",
    "                        input_data['image'][...,0:3],\n",
    "                        input_data['image'][...,6:9]])            \n",
    "\n",
    "            plt.imshow(img_3stack, alpha=1)\n",
    "            plt.imshow(salmap_3stack, alpha=0.5, cmap=plt.get_cmap('Reds'))\n",
    "            plt.axis('off')\n",
    "            writer.grab_frame() ; f.clear()\n",
    "\n",
    "            tstr = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - start))\n",
    "            print('\\ttime: {} | progress: {:.1f}%'.format(tstr, 100*i/total_frames), end='\\r')\n",
    "    print('\\nfinished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "pre_trained_swapnil32_leading.mp4 being processed\n",
      "\ttime: 01h 15m 46s | progress: 100.0%\n",
      "finished.\n",
      "<All keys matched successfully>\n",
      "imgmodel_17trainseqs_CC-1_LR-4_FT_swapnil32_leading.mp4 already exists\n",
      "<All keys matched successfully>\n",
      "pre_trained_brady54_leading.mp4 being processed\n",
      "\ttime: 01h 13m 41s | progress: 100.0%\n",
      "finished.\n",
      "<All keys matched successfully>\n",
      "imgmodel_17trainseqs_CC-1_LR-4_FT_brady54_leading.mp4 already exists\n",
      "<All keys matched successfully>\n",
      "pre_trained_tab32_leading.mp4 being processed\n",
      "\ttime: 01h 15m 37s | progress: 100.0%\n",
      "finished.\n",
      "<All keys matched successfully>\n",
      "imgmodel_17trainseqs_CC-1_LR-4_FT_tab32_leading.mp4 already exists\n",
      "<All keys matched successfully>\n",
      "pre_trained_dexter54_leading.mp4 being processed\n",
      "\ttime: 01h 13m 13s | progress: 100.0%\n",
      "finished.\n",
      "<All keys matched successfully>\n",
      "imgmodel_17trainseqs_CC-1_LR-4_FT_dexter54_leading.mp4 already exists\n",
      "<All keys matched successfully>\n",
      "pre_trained_tab54_leading.mp4 being processed\n",
      "\ttime: 01h 13m 13s | progress: 100.0%\n",
      "finished.\n",
      "<All keys matched successfully>\n",
      "imgmodel_17trainseqs_CC-1_LR-4_FT_tab54_leading.mp4 already exists\n",
      "<All keys matched successfully>\n",
      "pre_trained_alex54_leading.mp4 being processed\n",
      "\ttime: 01h 14m 50s | progress: 100.0%\n",
      "finished.\n",
      "<All keys matched successfully>\n",
      "imgmodel_17trainseqs_CC-1_LR-4_FT_alex54_leading.mp4 already exists\n",
      "<All keys matched successfully>\n",
      "pre_trained_jacob54_leading.mp4 being processed\n",
      "\ttime: 01h 14m 32s | progress: 100.0%\n",
      "finished.\n",
      "<All keys matched successfully>\n",
      "imgmodel_17trainseqs_CC-1_LR-4_FT_jacob54_leading.mp4 being processed\n",
      "\ttime: 01h 15m 11s | progress: 100.0%\n",
      "finished.\n",
      "<All keys matched successfully>\n",
      "pre_trained_esther54_leading.mp4 being processed\n",
      "\ttime: 00h 59m 18s | progress: 78.7%\r"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "data_root = Path(\"/scratch/abhijatb/Bosch22/dreyevr_recordings_sensors/LBC_config\")\n",
    "routedir_iterator = data_root.iterdir()\n",
    "# routedir_iterator = data_root.glob(\"*11*/\")\n",
    "path_to_conf_file = LBC_root / \"checkpoints/pre_trained/epoch_24.ckpt\"\n",
    "\n",
    "paths_to_conf_files = [LBC_root / \"checkpoints/pre_trained/epoch_24.ckpt\",\n",
    "                      LBC_root / \"checkpoints/imgmodel_17trainseqs_CC-1_LR-4_FT/epoch=27.ckpt\"]\n",
    "\n",
    "for route_data_path in routedir_iterator:\n",
    "    for path_to_conf_file in paths_to_conf_files:\n",
    "        saliency_movie(path_to_conf_file, route_data_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_data_path = route_dir\n",
    "\n",
    "final_datapt_idx = int(sorted(list(route_rgb_path.glob('*.png')))[-1].stem)\n",
    "datapt_idcs = np.arange(final_datapt_idx)+1 # starts from 0001.png\n",
    "\n",
    "sampled_dataidx = np.random.choice(datapt_idcs)\n",
    "input_data = get_data(route_data_path, sampled_dataidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model to introspect (interactive testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /scratch/abhijatb/Bosch22/LbC_DReyeVR/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_conf_file = LBC_root / \"checkpoints/imgmodel_17trainseqs_CC-1_LR-4_FT/epoch=27.ckpt\"\n",
    "dreyevr_img_agent = ImageAgent(str(path_to_conf_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=10\n",
    "r=10\n",
    "aggregate_method=\"leading\"\n",
    "smap = score_frame(dreyevr_img_agent, input_data, r, d, interp_func=occlude, pt_aggregate=aggregate_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show salience across images\n",
    "salmap_3stack = np.hstack([smap[:,:,1], smap[:,:,0], smap[:,:,2]])\n",
    "img_3stack = np.hstack([input_data['image'][...,0:3],\n",
    "                        input_data['image'][...,3:6],\n",
    "                        input_data['image'][...,6:9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=[10*3.2, 10], dpi=150)\n",
    "plt.imshow(img_3stack, alpha=1)\n",
    "plt.imshow(salmap_3stack, alpha=0.5, cmap=plt.get_cmap('Reds'))\n",
    "plt.show()\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "plt.imshow(smap[:,:,0])\n",
    "plt.show()\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_img =  input_data['image'][...,k*3:k*3+3] if k <3 else input_data['image'][...,-1]\n",
    "f = plt.figure()\n",
    "plt.imshow(input_img)\n",
    "plt.show()\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_img = masked_data['image'][...,k*3:k*3+3] if k <3 else masked_data['image'][...,-1]\n",
    "f = plt.figure()\n",
    "plt.imshow(masked_img)\n",
    "plt.show()\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,post_h,d):\n",
    "    for j in range(0,post_w,d):\n",
    "        for k in range(0,3): # this is for the channel rgb/left/right/target\n",
    "            break\n",
    "            mask = get_mask(center=[i,j], size=[post_h,post_w], r=r) # perturbation mask\n",
    "            masked_data = apply_mask(input_data, mask, interp_func, channel=k)\n",
    "            l = fwd_img_model(dreyevr_img_agent, masked_data)\n",
    "            scores[int(i/d),int(j/d), k] = (L-l).sum()\n",
    "# avoid range artifacts while resizing\n",
    "pmax = scores.max()\n",
    "scores = cv2.resize(scores, dsize=(post_h,post_w), interpolation=cv2.INTER_LINEAR).astype(np.float32)\n",
    "# return pmax * scores / scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = np.degrees(np.pi / 2 - np.arctan2(aim[1], aim[0])) / 90\n",
    "steer = self._turn_controller.step(angle)\n",
    "steer = np.clip(steer, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_logits = logits.view(logits.shape[:-2] + (-1,))\n",
    "flat_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_for_saliency(self, input_data):\n",
    "    tick_data = self.offline_tick(input_data)\n",
    "\n",
    "    img = torchvision.transforms.functional.to_tensor(tick_data['image'])\n",
    "    img = img[None].cuda()\n",
    "\n",
    "    target = torch.from_numpy(tick_data['target'])\n",
    "    target = target[None].cuda()\n",
    "\n",
    "    points, (target_cam, _) = self.net.forward(img, target)\n",
    "    points_cam = points.clone().cpu()\n",
    "    points_cam[..., 0] = (points_cam[..., 0] + 1) / 2 * img.shape[-1]\n",
    "    points_cam[..., 1] = (points_cam[..., 1] + 1) / 2 * img.shape[-2]\n",
    "    points_cam = points_cam.squeeze()\n",
    "    points_world = self.converter.cam_to_world(points_cam).numpy()\n",
    "\n",
    "    aim = (points_world[1] + points_world[0]) / 2.0\n",
    "    angle = np.degrees(np.pi / 2 - np.arctan2(aim[1], aim[0])) / 90\n",
    "    steer = self._turn_controller.step(angle)\n",
    "    steer = np.clip(steer, -1.0, 1.0)\n",
    "\n",
    "    desired_speed = np.linalg.norm(points_world[0] - points_world[1]) * 2.0\n",
    "    # desired_speed *= (1 - abs(angle)) ** 2\n",
    "\n",
    "    speed = tick_data['speed']\n",
    "\n",
    "    brake = desired_speed < 0.4 or (speed / desired_speed) > 1.1\n",
    "\n",
    "    delta = np.clip(desired_speed - speed, 0.0, 0.25)\n",
    "    throttle = self._speed_controller.step(delta)\n",
    "    throttle = np.clip(throttle, 0.0, 0.75)\n",
    "    throttle = throttle if not brake else 0.0\n",
    "\n",
    "    control = carla.VehicleControl()\n",
    "    control.steer = steer\n",
    "    control.throttle = throttle\n",
    "    control.brake = float(brake)\n",
    "\n",
    "    if DEBUG:\n",
    "        debug_display(\n",
    "                tick_data, target_cam.squeeze(), points.cpu().squeeze(),\n",
    "                steer, throttle, brake, desired_speed,\n",
    "                self.step)\n",
    "\n",
    "    return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_cam[..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_step(self, input_data, timestamp):\n",
    "    if not self.initialized:\n",
    "        self._init()\n",
    "\n",
    "    tick_data = self.tick(input_data)\n",
    "\n",
    "    img = torchvision.transforms.functional.to_tensor(tick_data['image'])\n",
    "    img = img[None].cuda()\n",
    "\n",
    "    target = torch.from_numpy(tick_data['target'])\n",
    "    target = target[None].cuda()\n",
    "\n",
    "    points, (target_cam, _) = self.net.forward(img, target)\n",
    "    points_cam = points.clone().cpu()\n",
    "    points_cam[..., 0] = (points_cam[..., 0] + 1) / 2 * img.shape[-1]\n",
    "    points_cam[..., 1] = (points_cam[..., 1] + 1) / 2 * img.shape[-2]\n",
    "    points_cam = points_cam.squeeze()\n",
    "    points_world = self.converter.cam_to_world(points_cam).numpy()\n",
    "\n",
    "    aim = (points_world[1] + points_world[0]) / 2.0\n",
    "    angle = np.degrees(np.pi / 2 - np.arctan2(aim[1], aim[0])) / 90\n",
    "    steer = self._turn_controller.step(angle)\n",
    "    steer = np.clip(steer, -1.0, 1.0)\n",
    "\n",
    "    desired_speed = np.linalg.norm(points_world[0] - points_world[1]) * 2.0\n",
    "    # desired_speed *= (1 - abs(angle)) ** 2\n",
    "\n",
    "    speed = tick_data['speed']\n",
    "\n",
    "    brake = desired_speed < 0.4 or (speed / desired_speed) > 1.1\n",
    "\n",
    "    delta = np.clip(desired_speed - speed, 0.0, 0.25)\n",
    "    throttle = self._speed_controller.step(delta)\n",
    "    throttle = np.clip(throttle, 0.0, 0.75)\n",
    "    throttle = throttle if not brake else 0.0\n",
    "\n",
    "    control = carla.VehicleControl()\n",
    "    control.steer = steer\n",
    "    control.throttle = throttle\n",
    "    control.brake = float(brake)\n",
    "    return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fwd_imgagent(img_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put in forward hooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load agent, build environment, play an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Breakout-v0'\n",
    "save_dir = 'figures/'\n",
    "\n",
    "print(\"set up dir variables and environment...\")\n",
    "load_dir = 'pretrained/{}/'.format(env_name.lower())\n",
    "meta = get_env_meta(env_name)\n",
    "env = gym.make(env_name) ; env.seed(1)\n",
    "\n",
    "print(\"initialize agent and try to load saved weights...\")\n",
    "model = NNPolicy(channels=1, num_actions=env.action_space.n)\n",
    "_ = model.try_load(load_dir, checkpoint='*.tar') ; torch.manual_seed(1)\n",
    "\n",
    "print(\"get a rollout of the policy...\")\n",
    "history = rollout(model, env, max_ep_len=3e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=[3,3*1.3])\n",
    "# frame_ix = 1404\n",
    "frame_ix=1404\n",
    "plt.imshow(history['ins'][frame_ix])\n",
    "for a in f.axes: a.get_xaxis().set_visible(False) ; a.get_yaxis().set_visible(False)\n",
    "plt.show(f)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Jacobian saliency map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(model, layer, top_dh, X):\n",
    "    global top_h_ ; top_h_ = None\n",
    "    def hook_top_h(m, i, o): global top_h_ ; top_h_ = o.clone()\n",
    "    hook1 = layer.register_forward_hook(hook_top_h)\n",
    "    _ = model(X) # do a forward pass so the forward hooks can be called\n",
    "\n",
    "    # backprop positive signal\n",
    "#     torch.autograd.backward(top_h_, top_dh.clone(), retain_variables=True) # backward hooks are called here\n",
    "    torch.autograd.backward(top_h_, top_dh.clone(), retain_graph=True) # backward hooks are called here\n",
    "    \n",
    "    hook1.remove()\n",
    "    return X[0].grad.data.clone().numpy(), X[0].data.clone().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative is simply the output policy distribution\n",
    "top_dh_actor = torch.Tensor(history['logits'][frame_ix]).view(1,-1)\n",
    "top_dh_critic = torch.Tensor(history['values'][frame_ix]).view(1,-1).fill_(1)\n",
    "\n",
    "# get input\n",
    "tens_state = torch.Tensor(prepro(history['ins'][frame_ix]))\n",
    "state = Variable(tens_state.unsqueeze(0), requires_grad=True)\n",
    "hx = Variable(torch.Tensor(history['hx'][frame_ix-1]).view(1,-1))\n",
    "cx = Variable(torch.Tensor(history['cx'][frame_ix-1]).view(1,-1))\n",
    "X = (state, (hx, cx))\n",
    "\n",
    "actor_jacobian, _ = jacobian(model, model.actor_linear, top_dh_actor, X)\n",
    "\n",
    "state.grad.mul_(0) ; X = (state, (hx, cx))\n",
    "critic_jacobian, _ = jacobian(model, model.critic_linear, top_dh_critic, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get perturbation saliency map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "radius = 5\n",
    "density = 5\n",
    "\n",
    "actor_saliency = score_frame(model, history, frame_ix, radius, density, interp_func=occlude, mode='actor')\n",
    "critic_saliency = score_frame(model, history, frame_ix, radius, density, interp_func=occlude, mode='critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample jacobian saliencies\n",
    "frame = history['ins'][frame_ix].squeeze().copy()\n",
    "frame = saliency_on_atari_frame((actor_jacobian**2).squeeze(), frame, fudge_factor=1, channel=2, sigma=0)\n",
    "jacobian_map = saliency_on_atari_frame((critic_jacobian**2).squeeze(), frame, fudge_factor=15, channel=0, sigma=0)\n",
    "\n",
    "# upsample perturbation saliencies\n",
    "frame = history['ins'][frame_ix].squeeze().copy()\n",
    "frame = saliency_on_atari_frame(actor_saliency, frame, fudge_factor=200, channel=2)\n",
    "perturbation_map = saliency_on_atari_frame(critic_saliency, frame, fudge_factor=100, channel=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=[11, 5*1.3], dpi=75)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(jacobian_map)\n",
    "plt.title('Jacobian', fontsize=30)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(perturbation_map)\n",
    "plt.title('Ours', fontsize=30)\n",
    "\n",
    "for a in f.axes: a.get_xaxis().set_visible(False) ; a.get_yaxis().set_visible(False)\n",
    "plt.show() #; f.savefig('./figures/jacobian-vs-perturb.png', bbox_inches='tight')\n",
    "f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
